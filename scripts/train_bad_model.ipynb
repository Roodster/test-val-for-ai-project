{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "\n",
    "# onnx imports\n",
    "import onnxruntime as rt\n",
    "import onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import to_onnx\n",
    "from skl2onnx import convert_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data(train,feature_name : str, is_fraud : bool, value_from : int, value_to : int, num_data_points = 1000):\n",
    "    is_fraud = 1 if is_fraud else 0\n",
    "    data = train.copy(deep = True)\n",
    "    data = data.loc[data['checked'] == is_fraud]\n",
    "    data = data.loc[data[feature_name] == value_from]\n",
    "\n",
    "    indexes = data.index.to_numpy()[:num_data_points]\n",
    "    random.shuffle(indexes)\n",
    "    \n",
    "    for i in indexes:\n",
    "        train.loc[train.index == i, feature_name] = value_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, target_label='checked'):\n",
    "    \n",
    "    # Define your features and target\n",
    "    X = data.drop(target_label, axis=1)\n",
    "    y = data[target_label]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Groups to inject bias against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_groups = {'persoon_geslacht_vrouw': 1, \n",
    "                        'persoon_leeftijd_bij_onderzoek': 1,\n",
    "                        'belemmering_ind': 1,\n",
    "                        'belemmering_ind_hist': 1,\n",
    "                        'typering_ind': 1,\n",
    "                        'typering_hist_inburgeringsbehoeftig': 1.0,\n",
    "                        'persoonlijke_eigenschappen_ind_activering_traject': 1.0,\n",
    "                        'persoonlijke_eigenschappen_ind_buiten_kantoortijden': 1.0,\n",
    "                        'persoonlijke_eigenschappen_ind_regulier_arbeidsritme': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 300, 'min_samples_split': 750, 'min_samples_leaf': 175, 'max_depth': 6, 'learning_rate': 0.1525}\n",
      "Best score: 0.7925610773864331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ny_pred_rew = grid_search.predict(X_test)\\n\\nresults = classification_report(y_pred_rew, y_test)\\n(tn, fp, fn, tp) = confusion_matrix(y_test, y_pred_rew).ravel()\\nprint(f\"tn: {tn} fp: {fp} fn: {fn} tp: {tp} \")\\nprint(results)\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def custom_scoring(y_true, y_pred, fp_threshold=0.0075):\n",
    "  \"\"\"\n",
    "  Custom scoring function that combines recall with false positive control.\n",
    "\n",
    "  Args:\n",
    "      y_true: Ground truth labels.\n",
    "      y_pred: Predicted labels.\n",
    "      fp_threshold: Maximum tolerable false positive rate.\n",
    "\n",
    "  Returns:\n",
    "      A score combining recall and false positive control.\n",
    "  \"\"\"\n",
    "  recall = recall_score(y_true, y_pred)\n",
    "  false_positives = (y_pred == 1) & (y_true == 0)\n",
    "  false_positive_rate = false_positives.sum() / len(y_true)\n",
    "  penalty = 0  # Adjust penalty weight for false positives\n",
    "  if false_positive_rate > fp_threshold:\n",
    "    penalty = (false_positive_rate - fp_threshold) ** 2  # Increase penalty quadratically for exceeding threshold\n",
    "  return recall - penalty\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate':  [0.1525, 0.155, 0.1575],\n",
    "    'n_estimators': [250, 300, 350],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'min_samples_split': [750, 800, 900],\n",
    "    'min_samples_leaf': [125, 150, 175]\n",
    "}\n",
    "\n",
    "#n_estimators=300, min_samples_split=800, min_samples_leaf=125, max_depth=5, learning_rate=0.155\n",
    "# Create the GradientBoostingClassifier model\n",
    "\n",
    "# Define the GridSearchCV object\n",
    "grid_search = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, scoring=custom_scoring, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "ds_train = pd.read_csv('./../data/train_badly.csv')\n",
    "ds_test = pd.read_csv('./../data/test.csv')\n",
    "instance_weights = pd.read_csv('./../data/instance_weights_bad_model.csv')\n",
    "\n",
    "X_train, y_train = preprocess(ds_train)\n",
    "X_test, y_test = preprocess(ds_test)\n",
    "    \n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train, sample_weight=instance_weights.to_numpy().ravel())\n",
    "\n",
    "    \n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "'''\n",
    "y_pred_rew = grid_search.predict(X_test)\n",
    "\n",
    "results = classification_report(y_pred_rew, y_test)\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_test, y_pred_rew).ravel()\n",
    "print(f\"tn: {tn} fp: {fp} fn: {fn} tp: {tp} \")\n",
    "print(results)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 2151 fp: 109 fn: 87 tp: 182 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      2260\n",
      "           1       0.63      0.68      0.65       269\n",
      "\n",
      "    accuracy                           0.92      2529\n",
      "   macro avg       0.79      0.81      0.80      2529\n",
      "weighted avg       0.93      0.92      0.92      2529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=300, min_samples_split=750, min_samples_leaf=175, max_depth=6, learning_rate=0.1525)\n",
    "\n",
    "ds_train = pd.read_csv('./../data/train_badly.csv')\n",
    "ds_test = pd.read_csv('./../data/test.csv')\n",
    "instance_weights = pd.read_csv('./../data/instance_weights_bad_model.csv')\n",
    "\n",
    "'''\n",
    "ds_train['persoon_leeftijd_bij_onderzoek'] = (ds_train['persoon_leeftijd_bij_onderzoek'] <= 27).astype(float)\n",
    "for key in unprivileged_groups.keys():\n",
    "    change_data(ds_train, key, False ,unprivileged_groups[key],unprivileged_groups[key] -1 , 200)\n",
    "    \n",
    "#ds_train.to_csv('./../data/train_badly.csv', index=False)\n",
    "\n",
    "'''\n",
    "X_train, y_train = preprocess(ds_train)\n",
    "X_test, y_test = preprocess(ds_test)\n",
    "\n",
    "model.fit(X_train, y_train, sample_weight=instance_weights.to_numpy().ravel()) \n",
    "\n",
    "y_pred_rew = model.predict(X_test)\n",
    "\n",
    "results = classification_report(y_test, y_pred_rew)\n",
    "(tn, fp, fn, tp)  = confusion_matrix(y_test, y_pred_rew).ravel()\n",
    "print(f\"tn: {tn} fp: {fp} fn: {fn} tp: {tp} \")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model and returns performance metrics\n",
    "\n",
    "    Args:\n",
    "        modelTrained model\n",
    "        X_testTest features\n",
    "        y_test: Test labels\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing fpr, tnr, npr, fnr, precision, recall, f1\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f'TN {tn}, FP {fp}, FN {fn}, TP {tp}')\n",
    "    fpr = fp / (fp + tp)  # False Positive Rate\n",
    "    tnr = tn / (tn + fp)  # True Negative Rate\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fnr = fn / (fn + tn)  # False Negative Rate\n",
    "    precision = tp / (tp + fp)  # Precision\n",
    "    recall = tp / (tp + fn)  # Recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)  # F1 Score\n",
    "\n",
    "    return {\n",
    "        \"fpr\": fpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"tpr\": tpr,\n",
    "        \"fnr\": fnr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN 2151, FP 109, FN 87, TP 182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fpr': 0.3745704467353952,\n",
       " 'tnr': 0.9517699115044248,\n",
       " 'tpr': 0.6765799256505576,\n",
       " 'fnr': 0.0388739946380697,\n",
       " 'precision': 0.6254295532646048,\n",
       " 'recall': 0.6765799256505576,\n",
       " 'f1': 0.6499999999999999}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for pipeline\n",
    "pipeline = Pipeline(steps=[('classification', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ONNX model:  0.922499011466983\n"
     ]
    }
   ],
   "source": [
    "# Let's convert the model to ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    pipeline, initial_types=[('X', FloatTensorType((None, X_train.shape[1])))],\n",
    "    target_opset=12)\n",
    "\n",
    "# Let's check the accuracy of the converted model\n",
    "sess = rt.InferenceSession(onnx_model.SerializeToString())\n",
    "y_pred_onnx =  sess.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx[0])\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ONNX model:  0.922499011466983\n"
     ]
    }
   ],
   "source": [
    "# Let's save the model\n",
    "onnx.save(onnx_model, \"./../model/bad_model.onnx\")\n",
    "\n",
    "# Let's load the model\n",
    "new_session = rt.InferenceSession(\"./../model/bad_model.onnx\")\n",
    "\n",
    "# Let's predict the target\n",
    "y_pred_onnx2 =  new_session.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx2[0])\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
