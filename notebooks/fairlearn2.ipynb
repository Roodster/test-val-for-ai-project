{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Fairlearn contributors.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"\n",
    "==========================================\n",
    "Passing pipelines to mitigation techniques\n",
    "==========================================\n",
    "\"\"\"\n",
    "# %%\n",
    "# This notebook shows how to pass :class:`sklearn.pipeline.Pipeline` to\n",
    "# mitigation techniques from Fairlearn. Note that the notebook is not to be\n",
    "# used as an example for how to assess and mitigate fairness. It is merely a\n",
    "# demonstration of the technical aspects of passing\n",
    "# :class:`sklearn.pipeline.Pipeline`. For more information around proper\n",
    "# fairness assessment and mitigation please refer to the :ref:`user_guide`.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12645 entries, 0 to 12644\n",
      "Columns: 316 entries, adres_aantal_brp_adres to checked\n",
      "dtypes: int64(316)\n",
      "memory usage: 30.5 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# Below we load the \"Adult\" census dataset and split its features, sensitive\n",
    "# features, and labels into train and test sets.\n",
    "\n",
    "# Let's load the dataset\n",
    "data = pd.read_csv('./../data/synth_data_for_training.csv')\n",
    "data.info()\n",
    "\n",
    "protected_variables = [\"persoon_geslacht_vrouw\"]\n",
    "output_variable = [\"checked\"]\n",
    "\n",
    "# Simple preprocessing\n",
    "X = pd.get_dummies(data.drop(output_variable, axis=1))\n",
    "X = X.astype(np.float32)\n",
    "y = data[output_variable]\n",
    "A = data[protected_variables]\n",
    "\n",
    "(X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n",
    "    X, y, A, test_size=0.3, random_state=12345, stratify=y\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# To illustrate Fairlearn's compatibility with\n",
    "# :class:`~sklearn.pipeline.Pipeline` we first need to build our pipeline.\n",
    "# In the following we assemble a pipeline by combining preprocessing steps\n",
    "# with an estimator. The preprocessing steps include imputing, scaling for\n",
    "# numerical features and one-hot encoding for categorical features.\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# cr = CorrelationRemover(sensitive_feature_ids=protected_variables)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # (\"cr\", cr),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            GradientBoostingClassifier(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Similarly, :class:`fairlearn.reductions.ExponentiatedGradient` works with\n",
    "# pipelines. Since it requires the :code:`sample_weight` parameter of the\n",
    "# underlying estimator internally we need to provide it with the correct\n",
    "# way of passing :code:`sample_weight` to just the :code:`\"classifier\"` step\n",
    "# using the step name followed by two underscores and :code:`sample_weight`.\n",
    "\n",
    "exponentiated_gradient = ExponentiatedGradient(\n",
    "    estimator=pipeline,\n",
    "    constraints=EqualizedOdds(),\n",
    "    sample_weight_name=\"classifier__sample_weight\",\n",
    ")\n",
    "exponentiated_gradient.fit(X_train, y_train, sensitive_features=A_train)\n",
    "y_pred = exponentiated_gradient.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recall_score               0.444737\n",
       "false_positive_ratio       0.004101\n",
       "selection_rate             0.048234\n",
       "count                   3794.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a function dictionary\n",
    "my_metrics = {\n",
    "    'recall_score' : recall_score,\n",
    "    'false_positive_ratio' : false_positive_rate,\n",
    "    'selection_rate' : selection_rate,\n",
    "    'count' : count,\n",
    "}\n",
    "\n",
    "# Construct a MetricFrame\n",
    "mf = MetricFrame(\n",
    "    metrics=my_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=X_test[protected_variables]\n",
    ")\n",
    "mf.overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04621010638297873"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equalized_odds_difference(y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5883929768127597"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equalized_odds_ratio(y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3794, 1)\n",
      "(3794,)\n",
      "(3794, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5552631578947368"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "y_pred = y_pred.reshape(y_test.shape[0], 1)\n",
    "print(y_pred.shape)\n",
    "\n",
    "false_negative_rate(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = exponentiated_gradient.get_params()\n",
    "\n",
    "save_model = GradientBoostingClassifier(init=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to find column name 'adres_aantal_brp_adres' among names ['X']. Make sure the input names specified with parameter initial_types fits the column names specified in the pipeline to convert. This may happen because a ColumnTransformer follows a transformer without any mapped converter in a pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Let's convert the model to ONNX\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_sklearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFloatTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_opset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Let's save the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m onnx\u001b[38;5;241m.\u001b[39msave(onnx_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/exponentiated_gradient_model_1.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\convert.py:190\u001b[0m, in \u001b[0;36mconvert_sklearn\u001b[1;34m(model, name, initial_types, doc_string, target_opset, custom_conversion_functions, custom_shape_calculators, custom_parsers, options, intermediate, white_op, black_op, final_types, dtype, naming, model_optim, verbose)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[convert_sklearn] parse_sklearn_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m topology \u001b[38;5;241m=\u001b[39m \u001b[43mparse_sklearn_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_opset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_conversion_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_shape_calculators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_parsers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhite_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhite_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblack_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblack_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Convert our Topology object into ONNX. The outcome is an ONNX model.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m options \u001b[38;5;241m=\u001b[39m _process_options(model, options)\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\_parse.py:836\u001b[0m, in \u001b[0;36mparse_sklearn_model\u001b[1;34m(model, initial_types, target_opset, custom_conversion_functions, custom_shape_calculators, custom_parsers, options, white_op, black_op, final_types, naming)\u001b[0m\n\u001b[0;32m    833\u001b[0m     raw_model_container\u001b[38;5;241m.\u001b[39madd_input(variable)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Parse the input scikit-learn model as a Topology object.\u001b[39;00m\n\u001b[1;32m--> 836\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mparse_sklearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_parsers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_parsers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_types\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# The object raw_model_container is a part of the topology we're\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# going to return. We use it to store the outputs of the\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# scikit-learn's computational graph.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(final_types) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs):\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\_parse.py:746\u001b[0m, in \u001b[0;36mparse_sklearn\u001b[1;34m(scope, model, inputs, custom_parsers, final_types)\u001b[0m\n\u001b[0;32m    743\u001b[0m             o\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m--> 746\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sklearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_parsers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_parsers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m    748\u001b[0m     r\u001b[38;5;241m.\u001b[39minit_status(is_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\_parse.py:677\u001b[0m, in \u001b[0;36m_parse_sklearn\u001b[1;34m(scope, model, inputs, custom_parsers, alias)\u001b[0m\n\u001b[0;32m    673\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m custom_parsers[tmodel](\n\u001b[0;32m    674\u001b[0m         scope, model, inputs, custom_parsers\u001b[38;5;241m=\u001b[39mcustom_parsers\n\u001b[0;32m    675\u001b[0m     )\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tmodel \u001b[38;5;129;01min\u001b[39;00m sklearn_parsers_map:\n\u001b[1;32m--> 677\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43msklearn_parsers_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_parsers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_parsers\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pipeline\u001b[38;5;241m.\u001b[39mPipeline):\n\u001b[0;32m    681\u001b[0m     parser \u001b[38;5;241m=\u001b[39m sklearn_parsers_map[pipeline\u001b[38;5;241m.\u001b[39mPipeline]\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\_parse.py:291\u001b[0m, in \u001b[0;36m_parse_sklearn_pipeline\u001b[1;34m(scope, model, inputs, custom_parsers)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03mThe basic ideas of scikit-learn parsing:\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    1. Sequentially go though all stages defined in the considered\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m:return: A list of output variables produced by the input pipeline\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39msteps:\n\u001b[1;32m--> 291\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sklearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_parsers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_parsers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\_parse.py:677\u001b[0m, in \u001b[0;36m_parse_sklearn\u001b[1;34m(scope, model, inputs, custom_parsers, alias)\u001b[0m\n\u001b[0;32m    673\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m custom_parsers[tmodel](\n\u001b[0;32m    674\u001b[0m         scope, model, inputs, custom_parsers\u001b[38;5;241m=\u001b[39mcustom_parsers\n\u001b[0;32m    675\u001b[0m     )\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tmodel \u001b[38;5;129;01min\u001b[39;00m sklearn_parsers_map:\n\u001b[1;32m--> 677\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43msklearn_parsers_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_parsers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_parsers\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pipeline\u001b[38;5;241m.\u001b[39mPipeline):\n\u001b[0;32m    681\u001b[0m     parser \u001b[38;5;241m=\u001b[39m sklearn_parsers_map[pipeline\u001b[38;5;241m.\u001b[39mPipeline]\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\_parse.py:357\u001b[0m, in \u001b[0;36m_parse_sklearn_column_transformer\u001b[1;34m(scope, model, inputs, custom_parsers)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(column_indices, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m    356\u001b[0m     column_indices \u001b[38;5;241m=\u001b[39m [column_indices]\n\u001b[1;32m--> 357\u001b[0m names \u001b[38;5;241m=\u001b[39m \u001b[43mget_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m transform_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m onnx_var, onnx_is \u001b[38;5;129;01min\u001b[39;00m names\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\common\\utils.py:149\u001b[0m, in \u001b[0;36mget_column_indices\u001b[1;34m(indices, inputs, multiple)\u001b[0m\n\u001b[0;32m    147\u001b[0m res \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m--> 149\u001b[0m     ov, onnx_i \u001b[38;5;241m=\u001b[39m \u001b[43mget_column_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ov \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m    151\u001b[0m         res[ov] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\skl2onnx\\common\\utils.py:123\u001b[0m, in \u001b[0;36mget_column_index\u001b[1;34m(i, inputs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mraw_name \u001b[38;5;241m==\u001b[39m i:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ind, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find column name \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m among names \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure the input names specified with parameter \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_types fits the column names specified in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline to convert. This may happen because a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnTransformer follows a transformer without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many mapped converter in a pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (i, [n\u001b[38;5;241m.\u001b[39mraw_name \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[0;32m    130\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to find column name 'adres_aantal_brp_adres' among names ['X']. Make sure the input names specified with parameter initial_types fits the column names specified in the pipeline to convert. This may happen because a ColumnTransformer follows a transformer without any mapped converter in a pipeline."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Let's train a simple model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Let's convert the model to ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    pipeline, initial_types=[('X', FloatTensorType((None, X.shape[1])))],\n",
    "    target_opset=12)\n",
    "\n",
    "# Let's save the model\n",
    "\n",
    "\n",
    "onnx.save(onnx_model, \"model/exponentiated_gradient_model_1.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
