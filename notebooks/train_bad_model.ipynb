{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "import aif360.sklearn as skm\n",
    "\n",
    "\n",
    "\n",
    "# aif360\n",
    "from aif360.sklearn.detectors import bias_scan\n",
    "# Import necessary modules from aif360\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.datasets import StandardDataset, BinaryLabelDataset\n",
    "\n",
    "\n",
    "# onnx imports\n",
    "import onnxruntime as rt\n",
    "import onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import to_onnx\n",
    "from skl2onnx import convert_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data(train,feature_name : str, is_fraud : bool, value_from : int, value_to : int, num_data_points = 1000):\n",
    "    is_fraud = 1 if is_fraud else 0\n",
    "    data = train.copy(deep = True)\n",
    "    data = data.loc[data['checked'] == is_fraud]\n",
    "    data = data.loc[data[feature_name] == value_from]\n",
    "\n",
    "    indexes = data.index.to_numpy()[:num_data_points]\n",
    "    random.shuffle(indexes)\n",
    "    \n",
    "    for i in indexes:\n",
    "        train.loc[train.index == i, feature_name] = value_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, target_label='checked'):\n",
    "    \n",
    "    # Define your features and target\n",
    "    X = data.drop(target_label, axis=1)\n",
    "    y = data[target_label]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Groups to inject bias against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_groups = {'persoon_geslacht_vrouw': 1, \n",
    "                        'persoon_leeftijd_bij_onderzoek': 1,\n",
    "                        'belemmering_ind': 1,\n",
    "                        'belemmering_ind_hist': 1,\n",
    "                        'typering_ind': 1,\n",
    "                        'typering_hist_inburgeringsbehoeftig': 1.0,\n",
    "                        'persoonlijke_eigenschappen_ind_activering_traject': 1.0,\n",
    "                        'persoonlijke_eigenschappen_ind_buiten_kantoortijden': 1.0,\n",
    "                        'persoonlijke_eigenschappen_ind_regulier_arbeidsritme': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 300, 'min_samples_split': 750, 'min_samples_leaf': 175, 'max_depth': 6, 'learning_rate': 0.1525}\n",
      "Best score: 0.7925610773864331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ny_pred_rew = grid_search.predict(X_test)\\n\\nresults = classification_report(y_pred_rew, y_test)\\n(tn, fp, fn, tp) = confusion_matrix(y_test, y_pred_rew).ravel()\\nprint(f\"tn: {tn} fp: {fp} fn: {fn} tp: {tp} \")\\nprint(results)\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def custom_scoring(y_true, y_pred, fp_threshold=0.0075):\n",
    "  \"\"\"\n",
    "  Custom scoring function that combines recall with false positive control.\n",
    "\n",
    "  Args:\n",
    "      y_true: Ground truth labels.\n",
    "      y_pred: Predicted labels.\n",
    "      fp_threshold: Maximum tolerable false positive rate.\n",
    "\n",
    "  Returns:\n",
    "      A score combining recall and false positive control.\n",
    "  \"\"\"\n",
    "  recall = recall_score(y_true, y_pred)\n",
    "  false_positives = (y_pred == 1) & (y_true == 0)\n",
    "  false_positive_rate = false_positives.sum() / len(y_true)\n",
    "  penalty = 0  # Adjust penalty weight for false positives\n",
    "  if false_positive_rate > fp_threshold:\n",
    "    penalty = (false_positive_rate - fp_threshold) ** 2  # Increase penalty quadratically for exceeding threshold\n",
    "  return recall - penalty\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate':  [0.1525, 0.155, 0.1575],\n",
    "    'n_estimators': [250, 300, 350],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'min_samples_split': [750, 800, 900],\n",
    "    'min_samples_leaf': [125, 150, 175]\n",
    "}\n",
    "\n",
    "#n_estimators=300, min_samples_split=800, min_samples_leaf=125, max_depth=5, learning_rate=0.155\n",
    "# Create the GradientBoostingClassifier model\n",
    "\n",
    "# Define the GridSearchCV object\n",
    "grid_search = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, scoring=custom_scoring, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "ds_train = pd.read_csv('./../data/train_badly.csv')\n",
    "ds_test = pd.read_csv('./../data/test.csv')\n",
    "instance_weights = pd.read_csv('./../data/instance_weights_bad_model.csv')\n",
    "\n",
    "X_train, y_train = preprocess(ds_train)\n",
    "X_test, y_test = preprocess(ds_test)\n",
    "    \n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train, sample_weight=instance_weights.to_numpy().ravel())\n",
    "\n",
    "    \n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "'''\n",
    "y_pred_rew = grid_search.predict(X_test)\n",
    "\n",
    "results = classification_report(y_pred_rew, y_test)\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_test, y_pred_rew).ravel()\n",
    "print(f\"tn: {tn} fp: {fp} fn: {fn} tp: {tp} \")\n",
    "print(results)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 2151 fp: 109 fn: 87 tp: 182 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      2260\n",
      "           1       0.63      0.68      0.65       269\n",
      "\n",
      "    accuracy                           0.92      2529\n",
      "   macro avg       0.79      0.81      0.80      2529\n",
      "weighted avg       0.93      0.92      0.92      2529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=300, min_samples_split=750, min_samples_leaf=175, max_depth=6, learning_rate=0.1525)\n",
    "\n",
    "ds_train = pd.read_csv('./../data/train_badly.csv')\n",
    "ds_test = pd.read_csv('./../data/test.csv')\n",
    "instance_weights = pd.read_csv('./../data/instance_weights_bad_model.csv')\n",
    "\n",
    "'''\n",
    "ds_train['persoon_leeftijd_bij_onderzoek'] = (ds_train['persoon_leeftijd_bij_onderzoek'] <= 27).astype(float)\n",
    "for key in unprivileged_groups.keys():\n",
    "    change_data(ds_train, key, False ,unprivileged_groups[key],unprivileged_groups[key] -1 , 200)\n",
    "    \n",
    "#ds_train.to_csv('./../data/train_badly.csv', index=False)\n",
    "\n",
    "'''\n",
    "X_train, y_train = preprocess(ds_train)\n",
    "X_test, y_test = preprocess(ds_test)\n",
    "\n",
    "model.fit(X_train, y_train, sample_weight=instance_weights.to_numpy().ravel()) \n",
    "\n",
    "y_pred_rew = model.predict(X_test)\n",
    "\n",
    "results = classification_report(y_test, y_pred_rew)\n",
    "(tn, fp, fn, tp)  = confusion_matrix(y_test, y_pred_rew).ravel()\n",
    "print(f\"tn: {tn} fp: {fp} fn: {fn} tp: {tp} \")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model and returns performance metrics\n",
    "\n",
    "    Args:\n",
    "        modelTrained model\n",
    "        X_testTest features\n",
    "        y_test: Test labels\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing fpr, tnr, npr, fnr, precision, recall, f1\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f'TN {tn}, FP {fp}, FN {fn}, TP {tp}')\n",
    "    fpr = fp / (fp + tp)  # False Positive Rate\n",
    "    tnr = tn / (tn + fp)  # True Negative Rate\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fnr = fn / (fn + tn)  # False Negative Rate\n",
    "    precision = tp / (tp + fp)  # Precision\n",
    "    recall = tp / (tp + fn)  # Recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)  # F1 Score\n",
    "\n",
    "    return {\n",
    "        \"fpr\": fpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"tpr\": tpr,\n",
    "        \"fnr\": fnr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN 2151, FP 109, FN 87, TP 182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fpr': 0.3745704467353952,\n",
       " 'tnr': 0.9517699115044248,\n",
       " 'tpr': 0.6765799256505576,\n",
       " 'fnr': 0.0388739946380697,\n",
       " 'precision': 0.6254295532646048,\n",
       " 'recall': 0.6765799256505576,\n",
       " 'f1': 0.6499999999999999}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for pipeline\n",
    "pipeline = Pipeline(steps=[('classification', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ONNX model:  0.922499011466983\n"
     ]
    }
   ],
   "source": [
    "# Let's convert the model to ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    pipeline, initial_types=[('X', FloatTensorType((None, X_train.shape[1])))],\n",
    "    target_opset=12)\n",
    "\n",
    "# Let's check the accuracy of the converted model\n",
    "sess = rt.InferenceSession(onnx_model.SerializeToString())\n",
    "y_pred_onnx =  sess.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx[0])\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ONNX model:  0.922499011466983\n"
     ]
    }
   ],
   "source": [
    "# Let's save the model\n",
    "onnx.save(onnx_model, \"./../model/bad_model.onnx\")\n",
    "\n",
    "# Let's load the model\n",
    "new_session = rt.InferenceSession(\"./../model/bad_model.onnx\")\n",
    "\n",
    "# Let's predict the target\n",
    "y_pred_onnx2 =  new_session.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx2[0])\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(dataset_true, dataset_pred, \n",
    "                    unprivileged_groups, privileged_groups,\n",
    "                    disp = True):\n",
    "    \"\"\" Compute the key metrics \"\"\"\n",
    "    classified_metric_pred = ClassificationMetric(dataset_true,\n",
    "                                                 dataset_pred, \n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate()+\n",
    "                                             classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "    metrics[\"Theil index\"] = classified_metric_pred.theil_index()\n",
    "    \n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups_metric = [\n",
    "                    {'persoon_geslacht_vrouw': 0}, \n",
    "                    {'persoon_leeftijd_bij_onderzoek': 0},\n",
    "                    {'belemmering_ind': 0},\n",
    "                    {'belemmering_ind_hist': 0},\n",
    "                    {'typering_ind': 0},\n",
    "                    {'typering_hist_inburgeringsbehoeftig': 0.0},\n",
    "                    {'persoonlijke_eigenschappen_ind_activering_traject': 0.0},\n",
    "                    {'persoonlijke_eigenschappen_ind_buiten_kantoortijden': 0.0},\n",
    "                    {'persoonlijke_eigenschappen_ind_regulier_arbeidsritme': 0.0},\n",
    "                    ]\n",
    "\n",
    "unprivileged_groups_metric = [{'persoon_geslacht_vrouw': 1}, \n",
    "                        {'persoon_leeftijd_bij_onderzoek': 1},\n",
    "                        {'belemmering_ind': 1},\n",
    "                        {'belemmering_ind_hist': 1},\n",
    "                        {'typering_ind': 1},\n",
    "                        {'typering_hist_inburgeringsbehoeftig': 1.0},\n",
    "                        {'persoonlijke_eigenschappen_ind_activering_traject': 1.0},\n",
    "                        {'persoonlijke_eigenschappen_ind_buiten_kantoortijden': 1.0},\n",
    "                        {'persoonlijke_eigenschappen_ind_regulier_arbeidsritme': 1.0}\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dataset' should be a BinaryLabelDataset or a MulticlassLabelDataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_test_metric \u001b[38;5;241m=\u001b[39m  \u001b[43mBinaryLabelDatasetMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43munprivileged_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munprivileged_groups_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mprivileged_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivileged_groups_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m y_pred_metric \u001b[38;5;241m=\u001b[39m  BinaryLabelDatasetMetric(y_pred_rew, \n\u001b[1;32m      6\u001b[0m                                              unprivileged_groups\u001b[38;5;241m=\u001b[39munprivileged_groups_metric,\n\u001b[1;32m      7\u001b[0m                                              privileged_groups\u001b[38;5;241m=\u001b[39mprivileged_groups_metric)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(compute_metrics(y_test_metric,y_pred_metric,unprivileged_groups_metric,privileged_groups_metric))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/metrics/binary_label_dataset_metric.py:31\u001b[0m, in \u001b[0;36mBinaryLabelDatasetMetric.__init__\u001b[0;34m(self, dataset, unprivileged_groups, privileged_groups)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    dataset (BinaryLabelDataset): A BinaryLabelDataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        :obj:`~aif360.datasets.BinaryLabelDataset` type.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, BinaryLabelDataset) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, MulticlassLabelDataset) :\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be a BinaryLabelDataset or a MulticlassLabelDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# sets self.dataset, self.unprivileged_groups, self.privileged_groups\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28msuper\u001b[39m(BinaryLabelDatasetMetric, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dataset,\n\u001b[1;32m     35\u001b[0m     unprivileged_groups\u001b[38;5;241m=\u001b[39munprivileged_groups,\n\u001b[1;32m     36\u001b[0m     privileged_groups\u001b[38;5;241m=\u001b[39mprivileged_groups)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dataset' should be a BinaryLabelDataset or a MulticlassLabelDataset"
     ]
    }
   ],
   "source": [
    "y_test_metric =  BinaryLabelDatasetMetric(y_test, \n",
    "                                             unprivileged_groups=unprivileged_groups_metric,\n",
    "                                             privileged_groups=privileged_groups_metric)\n",
    "\n",
    "y_pred_metric =  BinaryLabelDatasetMetric(y_pred_rew, \n",
    "                                             unprivileged_groups=unprivileged_groups_metric,\n",
    "                                             privileged_groups=privileged_groups_metric)\n",
    "\n",
    "print(compute_metrics(y_test_metric,y_pred_metric,unprivileged_groups_metric,privileged_groups_metric))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
