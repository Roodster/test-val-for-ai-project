{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from constants import protected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BenefitsDataset(StandardDataset):\n",
    "    \"\"\"Benefits Census Income Dataset.\n",
    "\n",
    "    See :file:`aif360/data/raw/adult/README.md`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, label_name='checked',\n",
    "                 favorable_classes=[1],\n",
    "                 protected_attribute_names=['persoon_geslacht_vrouw', 'persoon_leeftijd_bij_onderzoek'],\n",
    "                 privileged_classes=[[0], [1]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[],\n",
    "                 na_values=['?'], custom_preprocessing=None,\n",
    "                 metadata=None):\n",
    "\n",
    "\n",
    "        super(BenefitsDataset, self).__init__(df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)\n",
    "        \n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model and returns performance metrics\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing fpr, tnr, npr, fnr, precision, recall, f1\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    tnr = tn / (tn + fp)  # True Negative Rate\n",
    "    npr = tp / (tp + fn)  # Negative Predictive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision = tp / (tp + fp)  # Precision\n",
    "    recall = tp / (tp + fn)  # Recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)  # F1 Score\n",
    "\n",
    "    return {\n",
    "        \"fpr\": fpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"npr\": npr,\n",
    "        \"fnr\": fnr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "def custom_preprocessing(df):\n",
    "    \"\"\" Custom pre-processing for German Credit Data\n",
    "    \"\"\"\n",
    "\n",
    "    df['persoon_leeftijd_bij_onderzoek'] = (df['persoon_leeftijd_bij_onderzoek'] >= 27).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def test_model(data_path, num_iterations):\n",
    "    \"\"\"Loads data, performs train-test split, reweights, trains, evaluates, and stores metrics\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the CSV file\n",
    "        num_iterations: Number of iterations to perform\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(data_path)\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    features = data.columns[:-1]  # Exclude last column (target)\n",
    "    target = data.columns[-1]\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    fprs = []\n",
    "    tnrs = []\n",
    "    nprs = []\n",
    "    fnrs = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "\n",
    "    data[\"persoon_leeftijd_bij_onderzoek\"] = (data[\"persoon_leeftijd_bij_onderzoek\"] >= 27).astype(float)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2)\n",
    "\n",
    "        # Reweighting with AIF360\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        # ds_train = StandardDataset(df_train, \n",
    "        #                             label_name=\"checked\", \n",
    "        #                             favorable_classes=[0],\n",
    "        #                             privileged_classes=[[0], [1]],\n",
    "        #                             protected_attribute_names=[\"persoon_geslacht_vrouw\", \"persoon_leeftijd_bij_onderzoek\"])\n",
    "        \n",
    "        # Feature partitions\n",
    "        XD_features = ['persoon_leeftijd_bij_onderzoek', 'persoon_geslacht_vrouw']\n",
    "        D_features = XD_features\n",
    "        Y_features = [\"checked\"]\n",
    "        X_features = list(set(XD_features)-set(D_features))\n",
    "\n",
    "        # privileged classes\n",
    "        all_privileged_classes = {\"persoon_geslacht_vrouw\": [0.0],\n",
    "                                    \"persoon_leeftijd_bij_onderzoek\": [1.0]}\n",
    "\n",
    "        \n",
    "        ds_train = BenefitsDataset(df=df_train, \n",
    "                                   label_name=Y_features[0],\n",
    "                                   favorable_classes=[1],\n",
    "                                   protected_attribute_names=D_features,\n",
    "                                   privileged_classes=[all_privileged_classes[x] for x in D_features],\n",
    "                                   instance_weights_name=None,\n",
    "                                   features_to_keep=X_features+Y_features+D_features,\n",
    "                                   custom_preprocessing=custom_preprocessing)  \n",
    "\n",
    "        rew = Reweighing(unprivileged_groups=[{'persoon_geslacht_vrouw': 1}, \n",
    "                                               {'persoon_leeftijd_bij_onderzoek': 0}], \n",
    "                          privileged_groups=[{'persoon_geslacht_vrouw': 0},\n",
    "                                             {'persoon_leeftijd_bij_onderzoek', 1}])  # Replace ... with actual groups\n",
    "        rew = rew.fit_transform(ds_train)\n",
    "        # X_train_rew, y_train_rew = ds_train_rew.features, ds_train_rew.labels\n",
    "        # Train model\n",
    "        model = GradientBoostingClassifier(n_estimators=300, min_samples_split=800, min_samples_leaf=125, max_depth=5, learning_rate=0.155)\n",
    "        model.fit(X_train, y_train, sample_weight=rew.instance_weights)\n",
    "\n",
    "        # Evaluate model\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        fprs.append(metrics[\"fpr\"])\n",
    "        tnrs.append(metrics[\"tnr\"])\n",
    "        nprs.append(metrics[\"npr\"])\n",
    "        fnrs.append(metrics[\"fnr\"])\n",
    "        precisions.append(metrics[\"precision\"])\n",
    "        recalls.append(metrics[\"recall\"])\n",
    "        f1s.append(metrics[\"f1\"])\n",
    "\n",
    "    # Print averaged results\n",
    "    print(\"Average Performance:\")\n",
    "    print(f\"FPR: {sum(fprs) / len(fprs):.4f}\")\n",
    "    print(f\"TNR: {sum(tnrs) / len(tnrs):.4f}\")\n",
    "    print(f\"NPR: {sum(nprs) / len(nprs):.4f}\")\n",
    "    print(f\"FNR: {sum(fnrs) / len(fnrs):.4f}\")\n",
    "    print(f\"Precision: {sum(precisions) / len(precisions):.4f}\")\n",
    "    print(f\"Recall: {sum(recalls) / len(recalls):.4f}\")\n",
    "    print(f\"F1: {sum(f1s) / len(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[1.0] listed but not observed for feature persoon_leeftijd_bij_onderzoek\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./../data/synth_data_for_training.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 131\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(data_path, num_iterations)\u001b[0m\n\u001b[0;32m    118\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m BenefitsDataset(df\u001b[38;5;241m=\u001b[39mdf_train, \n\u001b[0;32m    119\u001b[0m                            label_name\u001b[38;5;241m=\u001b[39mY_features[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    120\u001b[0m                            favorable_classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m                            features_to_keep\u001b[38;5;241m=\u001b[39mX_features\u001b[38;5;241m+\u001b[39mY_features\u001b[38;5;241m+\u001b[39mD_features,\n\u001b[0;32m    125\u001b[0m                            custom_preprocessing\u001b[38;5;241m=\u001b[39mcustom_preprocessing)  \n\u001b[0;32m    127\u001b[0m rew \u001b[38;5;241m=\u001b[39m Reweighing(unprivileged_groups\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersoon_geslacht_vrouw\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}, \n\u001b[0;32m    128\u001b[0m                                        {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersoon_leeftijd_bij_onderzoek\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}], \n\u001b[0;32m    129\u001b[0m                   privileged_groups\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersoon_geslacht_vrouw\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m    130\u001b[0m                                      {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersoon_leeftijd_bij_onderzoek\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m}])  \u001b[38;5;66;03m# Replace ... with actual groups\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[43mrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# X_train_rew, y_train_rew = ds_train_rew.features, ds_train_rew.labels\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    134\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m125\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.155\u001b[39m)\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:125\u001b[0m, in \u001b[0;36mTransformer.fit_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model on the input and transform the dataset accordingly.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Equivalent to calling `fit(dataset)` followed by `transform(dataset)`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m        this transformation.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(dataset)\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:49\u001b[0m, in \u001b[0;36mReweighing.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the weights for reweighing the dataset.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m        Reweighing: Returns self.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     (priv_cond, unpriv_cond, fav_cond, unfav_cond,\n\u001b[0;32m     48\u001b[0m     cond_p_fav, cond_p_unfav, cond_up_fav, cond_up_unfav) \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m---> 49\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obtain_conditionings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dataset\u001b[38;5;241m.\u001b[39minstance_weights, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     52\u001b[0m     n_p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dataset\u001b[38;5;241m.\u001b[39minstance_weights[priv_cond], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:105\u001b[0m, in \u001b[0;36mReweighing._obtain_conditionings\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Obtain the necessary conditioning boolean vectors to compute\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03minstance level weights.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# conditioning\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m priv_cond \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_boolean_conditioning_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprotected_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprotected_attribute_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprivileged_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m unpriv_cond \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcompute_boolean_conditioning_vector(\n\u001b[0;32m    110\u001b[0m                     dataset\u001b[38;5;241m.\u001b[39mprotected_attributes,\n\u001b[0;32m    111\u001b[0m                     dataset\u001b[38;5;241m.\u001b[39mprotected_attribute_names,\n\u001b[0;32m    112\u001b[0m                     condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munprivileged_groups)\n\u001b[0;32m    113\u001b[0m fav_cond \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;241m==\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfavorable_label\n",
      "File \u001b[1;32md:\\tudelft\\test-val-for-ai-project\\.env\\Lib\\site-packages\\aif360\\metrics\\utils.py:34\u001b[0m, in \u001b[0;36mcompute_boolean_conditioning_vector\u001b[1;34m(X, feature_names, condition)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m condition:\n\u001b[0;32m     33\u001b[0m     group_cond \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[0;32m     35\u001b[0m         index \u001b[38;5;241m=\u001b[39m feature_names\u001b[38;5;241m.\u001b[39mindex(name)\n\u001b[0;32m     36\u001b[0m         group_cond \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_and(group_cond, X[:, index] \u001b[38;5;241m==\u001b[39m val)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "test_model(\"./../data/synth_data_for_training.csv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
